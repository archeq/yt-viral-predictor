{
 "cells": [
  {
   "cell_type": "code",
   "id": "983f0cce-8914-4ce4-92e0-13837749bfc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:15:02.620355800Z",
     "start_time": "2026-01-29T15:15:00.852112400Z"
    }
   },
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Current Directory: {os.getcwd()}\")\n",
    "print(f\"Files in Directory: {os.listdir('.')}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Current Directory: C:\\Users\\arche\\PycharmProjects\\yt-viral-predictor\n",
      "Files in Directory: ['.git', '.gitignore', '.idea', '.ipynb_checkpoints', '.venv', 'api.txt', 'data', 'docker-compose.gpu.yml', 'docker-compose.yml', 'Dockerfile', 'download.py', 'LICENSE', 'main.py', 'nb.ipynb', 'README.md', 'requirements.gpu.txt', 'requirements.txt', 'thumbnails']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2c925bf1-565e-4275-934c-0bf1459b689c",
   "metadata": {},
   "source": "print(torch.cuda.is_available())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:15:06.676736600Z",
     "start_time": "2026-01-29T15:15:05.343438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"api.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "from IPython.display import JSON\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "def main(id_list):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=','.join(id_list)\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return JSON(response)\n",
    "\n",
    "channel_ids = [\n",
    "    'UCgKFOz_KrMbmypWrawtzDQg',\n",
    "]\n",
    "\n",
    "display(main(channel_ids))"
   ],
   "id": "a579e85926429c1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ],
      "application/json": {
       "kind": "youtube#channelListResponse",
       "etag": "EgmCfQ7QeFzOJLt_wn1pf2XNjaw",
       "pageInfo": {
        "totalResults": 1,
        "resultsPerPage": 5
       },
       "items": [
        {
         "kind": "youtube#channel",
         "etag": "Ve-joJiZXRSSzHS3i65NE_s-uY8",
         "id": "UCgKFOz_KrMbmypWrawtzDQg",
         "snippet": {
          "title": "Erik Cupsa",
          "description": "Welcome to the channel!\n\nThis is where I document the highs, lows, and everything in between of building my startup, Empor, from my dorm room. Along the way, I share actionable insights, lessons learned, and strategies for turning ideas into reality.\n\nI also vlog my journey as a software engineer and navigating life, balancing the grind, growth, and everything in between.\n\nIf you're an entrepreneur or just someone chasing big goals, this channel is here to inspire, educate, and keep it real.\n\nHit subscribe and join a community of driven creators and builders making things happen.\n",
          "customUrl": "@swerikcodes",
          "publishedAt": "2023-07-19T04:42:59.663153Z",
          "thumbnails": {
           "default": {
            "url": "https://yt3.ggpht.com/JXUMAs_77dmgZyfoZ2hNH3wgbQSk5N5cSuFctiBMQJDQR-I0wWVGJQqhb4H1eOOtmEte-57szg=s88-c-k-c0x00ffffff-no-rj",
            "width": 88,
            "height": 88
           },
           "medium": {
            "url": "https://yt3.ggpht.com/JXUMAs_77dmgZyfoZ2hNH3wgbQSk5N5cSuFctiBMQJDQR-I0wWVGJQqhb4H1eOOtmEte-57szg=s240-c-k-c0x00ffffff-no-rj",
            "width": 240,
            "height": 240
           },
           "high": {
            "url": "https://yt3.ggpht.com/JXUMAs_77dmgZyfoZ2hNH3wgbQSk5N5cSuFctiBMQJDQR-I0wWVGJQqhb4H1eOOtmEte-57szg=s800-c-k-c0x00ffffff-no-rj",
            "width": 800,
            "height": 800
           }
          },
          "localized": {
           "title": "Erik Cupsa",
           "description": "Welcome to the channel!\n\nThis is where I document the highs, lows, and everything in between of building my startup, Empor, from my dorm room. Along the way, I share actionable insights, lessons learned, and strategies for turning ideas into reality.\n\nI also vlog my journey as a software engineer and navigating life, balancing the grind, growth, and everything in between.\n\nIf you're an entrepreneur or just someone chasing big goals, this channel is here to inspire, educate, and keep it real.\n\nHit subscribe and join a community of driven creators and builders making things happen.\n"
          }
         },
         "contentDetails": {
          "relatedPlaylists": {
           "likes": "",
           "uploads": "UUgKFOz_KrMbmypWrawtzDQg"
          }
         },
         "statistics": {
          "viewCount": "15777745",
          "subscriberCount": "88500",
          "hiddenSubscriberCount": false,
          "videoCount": "711"
         }
        }
       ]
      }
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:15:11.280581300Z",
     "start_time": "2026-01-29T15:15:10.831085400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "API_KEY = api_key\n",
    "VIDEO_ID = \"3fpqMJCoUQ0\" # Example ID\n",
    "\n",
    "def get_thumbnail_api(video_id, api_key):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # Request video details\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract the highest resolution URL available\n",
    "    thumbnails = response['items'][0]['snippet']['thumbnails']\n",
    "\n",
    "    # Try to get maxres, then high, then default\n",
    "    if 'maxres' in thumbnails:\n",
    "        return thumbnails['maxres']['url']\n",
    "    return thumbnails.get('high', {}).get('url')\n",
    "\n",
    "thumbnail_url = get_thumbnail_api(VIDEO_ID, API_KEY)\n",
    "print(f\"Found URL: {thumbnail_url}\")\n",
    "\n",
    "\n",
    "def download_image(url, filename=\"thumbnail.jpg\"):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Success! Image saved as {filename}\")\n",
    "    else:\n",
    "        print(\"Failed to download image.\")\n"
   ],
   "id": "9aa9c90a6c955771",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL: https://i.ytimg.com/vi/3fpqMJCoUQ0/maxresdefault.jpg\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "download_image(thumbnail_url, \"data/raw/thumbnails/test.jpg\")",
   "id": "e19771db14272c43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:15:25.461978100Z",
     "start_time": "2026-01-29T15:15:24.942489300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "PLAYLIST_ID = \"PLcEN1HW6GYIczNHmEUSKkJ646Ev3-i0A2\"\n",
    "\n",
    "def get_video_ids_from_playlist(playlist_id, api_key):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Request a page of playlist items\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Extract Video IDs from the current page\n",
    "        for item in response.get('items', []):\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "        # Check if there's another page\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Usage\n",
    "all_ids = get_video_ids_from_playlist(PLAYLIST_ID, API_KEY)\n",
    "print(f\"Retrieved {len(all_ids)} video IDs.\")\n",
    "print(all_ids[:5])  # Show first five"
   ],
   "id": "918705fbfc1ec63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 12 video IDs.\n",
      "['xx4vDZdRWDU', 'CHzha4Fxlg0', 'JGWabJWC24U', 'gaKUpWq5Qas', 'HWRZRfmrxRc']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for id in all_ids:\n",
    "    download_image(get_thumbnail_api(id, API_KEY), f\"data/raw/thumbnails/{id}.jpg\")"
   ],
   "id": "fbba08213d7d7ff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "CHANNEL_ID = channel_ids[0]\n",
    "\n",
    "def get_all_video_ids(channel_id, api_key):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # STEP 1: Get the 'uploads' playlist ID\n",
    "    ch_request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    ch_response = ch_request.execute()\n",
    "\n",
    "    # The ID typically looks like the channel ID but starts with 'UU' instead of 'UC'\n",
    "    uploads_playlist_id = ch_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    # STEP 2: Iterate through the playlist items\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        pl_request = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        pl_response = pl_request.execute()\n",
    "\n",
    "        for item in pl_response.get('items', []):\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "        next_page_token = pl_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "# Usage\n",
    "ids = get_all_video_ids(CHANNEL_ID, API_KEY)\n",
    "print(f\"Found {len(ids)} videos.\")"
   ],
   "id": "1fa931fb15c12ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:29:41.489289200Z",
     "start_time": "2026-01-29T15:29:19.397010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import yt_dlp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# API_KEY = api_key\n",
    "CHANNEL_ID = channel_ids[0]\n",
    "OUTPUT_FOLDER = \"data/raw/thumbnails\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "def get_long_form_videos(channel_id):\n",
    "    \"\"\"\n",
    "    Uses yt-dlp to scrape ONLY the 'Videos' tab (Long-form).\n",
    "    This automatically excludes anything YouTube considers a 'Short'.\n",
    "    \"\"\"\n",
    "    # We specifically target the /videos endpoint\n",
    "    channel_url = f\"https://www.youtube.com/channel/{channel_id}/videos\"\n",
    "\n",
    "    print(f\"Fetching long-form list from: {channel_url}\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'extract_flat': True, # FAST: Only grabs metadata, doesn't download video\n",
    "        'sleep_interval': 1,  # Be polite to avoid blocking\n",
    "    }\n",
    "\n",
    "    long_form_videos = []\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            # Extract info from the specific tab\n",
    "            info = ydl.extract_info(channel_url, download=False)\n",
    "\n",
    "            # The 'entries' key contains the videos\n",
    "            if 'entries' in info:\n",
    "                # We convert the generator to a list to count them\n",
    "                for entry in info['entries']:\n",
    "                    long_form_videos.append({\n",
    "                        'id': entry['id'],\n",
    "                        'title': entry['title'],\n",
    "                        # yt-dlp usually finds the best thumbnail automatically\n",
    "                        'thumbnail': entry.get('thumbnails', [{}])[-1].get('url')\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "\n",
    "    return long_form_videos\n",
    "\n",
    "def download_thumbnail(video):\n",
    "    \"\"\"Downloads the thumbnail for a single video.\"\"\"\n",
    "    try:\n",
    "        vid_id = video['id']\n",
    "        url = video['thumbnail']\n",
    "\n",
    "        # If yt-dlp didn't give a direct URL, construct the maxres one\n",
    "        if not url:\n",
    "            url = f\"https://img.youtube.com/vi/{vid_id}/maxresdefault.jpg\"\n",
    "\n",
    "        response = requests.get(url, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            filename = os.path.join(OUTPUT_FOLDER, f\"{vid_id}.jpg\")\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    # 1. Get the Clean List (Long-form only)\n",
    "    videos = get_long_form_videos(CHANNEL_ID)\n",
    "    print(f\"Found {len(videos)} videos in the 'Videos' tab.\")\n",
    "\n",
    "    # 2. Download Images\n",
    "    print(\"Downloading thumbnails...\")\n",
    "    success_count = 0\n",
    "\n",
    "    # Using a simple loop with progress bar (Threads usually not needed for <1000 items here)\n",
    "    for video in tqdm(videos):\n",
    "        if download_thumbnail(video):\n",
    "            success_count += 1\n",
    "\n",
    "    print(f\"\\nDone! Saved {success_count} thumbnails to '{OUTPUT_FOLDER}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "5bf5fb643b7b4eff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "OUTPUT_FOLDER = \"data/raw/thumbnails\"\n",
    "DATA_FILE = \"data/raw/video_data.csv\""
   ],
   "id": "4473f42f57a27cf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T15:49:41.229953300Z",
     "start_time": "2026-01-29T15:49:18.306297100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import yt_dlp\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_FOLDER = \"data/raw/thumbnails\"\n",
    "DATA_FILE = \"data/raw/video_data.csv\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "def get_long_form_data(channel_id):\n",
    "    channel_url = f\"https://www.youtube.com/channel/{channel_id}/videos\"\n",
    "    print(f\"Fetching metadata from: {channel_url}...\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,\n",
    "        'sleep_interval': 1,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(channel_url, download=False)\n",
    "            if 'entries' in info:\n",
    "                return list(info['entries'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def calculate_rolling_average(all_videos, current_index, window=10):\n",
    "    \"\"\"\n",
    "    Calculates avg views of previous videos.\n",
    "    Returns None if there is NO history (first video).\n",
    "    \"\"\"\n",
    "    start_idx = current_index + 1\n",
    "    end_idx = start_idx + window\n",
    "\n",
    "    # Get previous videos (slice handles out-of-bounds automatically)\n",
    "    previous_videos = all_videos[start_idx : end_idx]\n",
    "\n",
    "    # Filter valid view counts\n",
    "    views = [v.get('view_count') for v in previous_videos if v.get('view_count') is not None]\n",
    "\n",
    "    if not views:\n",
    "        return None # No history available (First video)\n",
    "\n",
    "    return statistics.mean(views)\n",
    "\n",
    "def download_thumbnail(video_id, url):\n",
    "    try:\n",
    "        if not url:\n",
    "            url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(OUTPUT_FOLDER, f\"{video_id}.jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    videos = get_long_form_data(CHANNEL_ID)\n",
    "    print(f\"Found {len(videos)} videos. Processing...\")\n",
    "\n",
    "    with open(DATA_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Updated Header: No Thumbnail File, Added VScore\n",
    "        writer.writerow(['Video ID', 'Title', 'Current Views', 'Avg Views (Last 10)', 'Virality Score'])\n",
    "\n",
    "        for i, video in tqdm(enumerate(videos), total=len(videos)):\n",
    "            vid_id = video.get('id')\n",
    "            title = video.get('title')\n",
    "            current_views = video.get('view_count')\n",
    "            thumb_url = video.get('thumbnails', [{}])[-1].get('url')\n",
    "\n",
    "            if not vid_id or current_views is None:\n",
    "                continue\n",
    "\n",
    "            # 1. Calculate Stats\n",
    "            avg_prev = calculate_rolling_average(videos, i, window=10)\n",
    "\n",
    "            # 2. Calculate Virality Score\n",
    "            # Formula: Current / Average\n",
    "            if avg_prev and avg_prev > 0:\n",
    "                v_score = current_views / avg_prev\n",
    "                avg_str = f\"{avg_prev:.0f}\"\n",
    "                score_str = f\"{v_score:.2f}\" # e.g., 1.50 (1.5x better than avg)\n",
    "            else:\n",
    "                # Handle the \"First Video\" or \"Zero History\" case\n",
    "                avg_str = \"N/A\"\n",
    "                score_str = \"N/A\"\n",
    "\n",
    "            # 3. Download Thumbnail (Image only)\n",
    "            download_thumbnail(vid_id, thumb_url)\n",
    "\n",
    "            # 4. Save Data\n",
    "            writer.writerow([\n",
    "                vid_id,\n",
    "                title,\n",
    "                current_views,\n",
    "                avg_str,\n",
    "                score_str\n",
    "            ])\n",
    "\n",
    "    print(f\"\\nDone! Data saved in '{DATA_FILE}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "ffc92ccf5cb04740",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T17:40:39.427623100Z",
     "start_time": "2026-01-29T17:40:22.514526300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "import yt_dlp\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NEIGHBOR_COUNT = 4  # Total neighbors to analyze (5 before + 5 after)\n",
    "OUTPUT_FOLDER = \"data/raw/thumbnails\"\n",
    "CHANNEL_ID = 'UCgKFOz_KrMbmypWrawtzDQg'\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "def get_long_form_data(channel_id):\n",
    "    \"\"\"Fetch all video metadata.\"\"\"\n",
    "    channel_url = f\"https://www.youtube.com/channel/{channel_id}/videos\"\n",
    "    print(f\"Fetching metadata from: {channel_url}...\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'extract_flat': True,\n",
    "        'sleep_interval': 1,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info = ydl.extract_info(channel_url, download=False)\n",
    "            if 'entries' in info:\n",
    "                return list(info['entries'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def calculate_elastic_average(all_videos, current_index, neighbor_target=10):\n",
    "    \"\"\"\n",
    "    Calculates average of 'neighbor_target' surrounding videos.\n",
    "    Elastic Logic: Tries to get 5 before and 5 after.\n",
    "    If not enough on one side, takes more from the other.\n",
    "    \"\"\"\n",
    "    total_videos = len(all_videos)\n",
    "    if total_videos <= 1:\n",
    "        return None\n",
    "\n",
    "    # 1. Define the ideal 'span' (Current video + neighbors)\n",
    "    # If we want 10 neighbors, we need a span of 11 videos total.\n",
    "    span_size = neighbor_target + 1\n",
    "\n",
    "    # 2. Calculate ideal start point (centered)\n",
    "    # For 10 neighbors, we want to start 5 slots back.\n",
    "    half_window = neighbor_target // 2\n",
    "    start_idx = current_index - half_window\n",
    "\n",
    "    # 3. Elastic Adjustments\n",
    "    # A. If start is negative (too new), shift window right to start at 0\n",
    "    if start_idx < 0:\n",
    "        start_idx = 0\n",
    "\n",
    "    # B. Calculate end index based on start\n",
    "    end_idx = start_idx + span_size\n",
    "\n",
    "    # C. If end exceeds list (too old), shift window left to fit\n",
    "    if end_idx > total_videos:\n",
    "        end_idx = total_videos\n",
    "        # Try to push start back to maintain span size\n",
    "        start_idx = max(0, end_idx - span_size)\n",
    "\n",
    "    # 4. Collect views from the window, SKIPPING the current video\n",
    "    neighbor_views = []\n",
    "\n",
    "    # Loop from start to end (exclusive)\n",
    "    for i in range(start_idx, end_idx):\n",
    "        if i == current_index:\n",
    "            continue\n",
    "\n",
    "        v = all_videos[i].get('view_count')\n",
    "        if v is not None:\n",
    "            neighbor_views.append(v)\n",
    "\n",
    "    if not neighbor_views:\n",
    "        return None\n",
    "\n",
    "    return statistics.mean(neighbor_views)\n",
    "\n",
    "def download_thumbnail(video_id, url):\n",
    "    try:\n",
    "        if not url:\n",
    "            url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(OUTPUT_FOLDER, f\"{video_id}.jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def main():\n",
    "    videos = get_long_form_data(CHANNEL_ID)\n",
    "    print(f\"Found {len(videos)} videos. Processing...\")\n",
    "\n",
    "    with open(f'data/raw/{CHANNEL_ID}.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Video ID', 'Title', 'Current Views', 'Elastic Avg', 'V-Score'])\n",
    "\n",
    "        for i, video in tqdm(enumerate(videos), total=len(videos)):\n",
    "            vid_id = video.get('id')\n",
    "            title = video.get('title')\n",
    "            current_views = video.get('view_count')\n",
    "            thumb_url = video.get('thumbnails', [{}])[-1].get('url')\n",
    "\n",
    "            if not vid_id or current_views is None:\n",
    "                continue\n",
    "\n",
    "            # 1. Calculate Elastic Average\n",
    "            elastic_avg = calculate_elastic_average(videos, i, NEIGHBOR_COUNT)\n",
    "\n",
    "            # 2. Calculate Score\n",
    "            if elastic_avg and elastic_avg > 0:\n",
    "                v_score = current_views / elastic_avg # V-Score == Virality Score\n",
    "                avg_str = f\"{elastic_avg:.0f}\"\n",
    "                score_str = f\"{v_score:.2f}\"\n",
    "            else:\n",
    "                avg_str = \"N/A\"\n",
    "                score_str = \"N/A\"\n",
    "\n",
    "            # 3. Download & Save\n",
    "            download_thumbnail(vid_id, thumb_url)\n",
    "\n",
    "            writer.writerow([\n",
    "                vid_id,\n",
    "                title,\n",
    "                current_views,\n",
    "                avg_str,\n",
    "                score_str\n",
    "            ])\n",
    "\n",
    "    print(f\"\\nDone! Data saved in '{CHANNEL_ID}.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "1521bfc95451d6bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T17:48:37.689205300Z",
     "start_time": "2026-01-29T17:48:35.911022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NICHES = [\"technology\", \"programming\", \"software engineering\"]\n",
    "MAX_CHANNELS_PER_NICHE = 10  # How many results to fetch per keyword\n",
    "\n",
    "with open(\"api.txt\", \"r\") as f:\n",
    "    API_KEY = f.read().strip()\n",
    "\n",
    "def search_channels_by_niche(api_key, niches, max_results=10):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # We use a dictionary to automatically handle duplicates\n",
    "    # Key = Channel ID, Value = Channel Title\n",
    "    unique_channels = {}\n",
    "\n",
    "    print(f\"Starting search for {len(niches)} niches...\\n\")\n",
    "\n",
    "    for niche in niches:\n",
    "        print(f\"üîé Searching for: '{niche}'...\")\n",
    "\n",
    "        try:\n",
    "            # The search().list() endpoint is the standard way to find channels\n",
    "            request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                type=\"channel\",       # strictly search for channels\n",
    "                q=niche,              # the search term\n",
    "                maxResults=max_results,\n",
    "                order=\"relevance\"     # usually finds the most \"popular\" for that term\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get('items', []):\n",
    "                channel_id = item['snippet']['channelId']\n",
    "                channel_title = item['snippet']['title']\n",
    "\n",
    "                # LOGIC: Check for duplicates\n",
    "                if channel_id not in unique_channels:\n",
    "                    unique_channels[channel_id] = channel_title\n",
    "                    print(f\"   ‚úÖ Added: {channel_title}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Duplicate ignored: {channel_title}\")\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"   ‚ùå API Error: {e}\")\n",
    "\n",
    "    return unique_channels"
   ],
   "id": "f0ba2386670dbdfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Added: Modern Software Engineering\n",
      "   ‚úÖ Added: Software Engineering\n",
      "   ‚úÖ Added: Software Engineering\n",
      "   ‚úÖ Added: Software Engineering\n",
      "   ‚úÖ Added: Software Engineering Institute | Carnegie Mellon University\n",
      "   ‚úÖ Added: Software Engineering Daily\n",
      "   ‚úÖ Added: Software Engineering\n",
      "   ‚úÖ Added: Software Engineering - ALX\n",
      "   ‚úÖ Added: Software Developer Diaries\n",
      "   ‚úÖ Added: HNU Software Engineering\n",
      "\n",
      "==============================\n",
      "Final Collection: 45 Unique Channels\n",
      "==============================\n",
      "UCy0tKL1T7wFoYcxCe0xjN6Q : Technology Connections\n",
      "UCUuvZ0skL2WYZ3rhPMZbfdA : Brick Technology\n",
      "UCAL3JXZSzSm8AlZyD3nQdBA : Primitive Technology\n",
      "UCrM7B7SL_g1edFOnmj-SDKg : Bloomberg Technology\n",
      "UCYVU6rModlGxvJbszCclGGw : Rob Braxman Tech\n",
      "UCf9phz2kU2DaJBwDATTD05Q : House Technology\n",
      "UCKWaEZ-_VweaEx1j62do_vQ : IBM Technology\n",
      "UCYUPLUCkMiUgiyVuluCc7tQ : Technology for Teachers and Students\n",
      "UCiz26UeGvcTy4_M3Zhgk7FQ : Defog Tech\n",
      "UCz6PEeVLG1TL6jMRTvSLm4g : Matt Talks Tech\n",
      "UCKnaflu7tLEm2S5yHBYfmtg : Complete Technology\n",
      "UC3ByF8DcZ3yxUs7VP1NOuyA : REWA Technology\n",
      "UCF8H7dYHK6AvJF0EVonO3cw : Playful Technology\n",
      "UC4Tklxku1yPcRIH0VVCKoeA : Quantum Tech HD\n",
      "UCRzScB0a-dc6OCYYZlP9qpA : Tech Notice\n",
      "UCs6N4AXdN9_7LAZQhkJ3ftA : MEGA Technology\n",
      "UCTR22zaU51pb2P72JS8oJtg : Velian Speaks Tech\n",
      "UC0QRG-gMqHhF5-BPdmJfs1w : TT Technology\n",
      "UC1tVU8H153ZFO9eRsxdJlhA : Technology Gyan\n",
      "UC905GJVaT_10oSdZq8sDoRg : Tech Excellence\n",
      "UCagoIYmo_gO1iVaCeeSikEg : This Week in Tech\n",
      "UCFnKqecC7F2HTnWZlEzExbw : Video Blog Tech\n",
      "UC08uERSqTFS3xSwXKMLrUVg : TECH TITANS\n",
      "UC7FWqacDz5A5bhbjxXbRN2Q : Technology Trends\n",
      "UC-42e9KDEWXNYJAT7qPWbeA : Primary Technology\n",
      "UCuEOSK8blSM6j5jxVp3ttnQ : Programming Advices\n",
      "UCMN0a7GHQnC6H74SmCGSmdw : You Suck at Programming\n",
      "UCWv7vMbMWH4-V0ZXdmDpPBA : Programming with Mosh\n",
      "UCBsAE4_Cvr7FGnF6KVw9JfA : hafif programming\n",
      "UCfJyQ3P2k_SuqfxVdqIEQNw : R Programming 101\n",
      "UCpKb02FsH4WH4X_2xhIoJ1A : The Audio Programmer\n",
      "UC42pOSNg804f1wCcj7qL0mA : Coding with John\n",
      "UCV7cZwHMX_0vk8DSYrS7GCg : Learn Coding\n",
      "UCvGwM5woTl13I-qThI4YMCg : Josh tried coding\n",
      "UCLZIElNyubHOvbfudT7KS1A : The V Programming Language\n",
      "UCCfqyGl3nq_V0bo64CjZh8g : Modern Software Engineering\n",
      "UCKvjEXuc-tomU0sW_iBZ3fw : Software Engineering\n",
      "UCzOb0YO5lm4AAHXC5VMy3-g : Software Engineering\n",
      "UCC05kqMzbQUqpAf_kqeOx0A : Software Engineering\n",
      "UCrmnnE3yzpAyAuX_hRqyLdg : Software Engineering Institute | Carnegie Mellon University\n",
      "UCBKM3HATRILZyG9jGNJKGXQ : Software Engineering Daily\n",
      "UCocTdfI3p1tKDQEbQzDWrPg : Software Engineering\n",
      "UCIZFrMwq1lXOA2899t7d5Aw : Software Engineering - ALX\n",
      "UCqAL_b-jUOTPjrTSNl2SNaQ : Software Developer Diaries\n",
      "UCUGjG_06mOK_6vkHpUi1Y4Q : HNU Software Engineering\n",
      "\n",
      "Saved IDs to 'channel_ids.txt'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T17:48:42.079284600Z",
     "start_time": "2026-01-29T17:48:40.363898700Z"
    }
   },
   "cell_type": "code",
   "source": "channels = search_channels_by_niche(API_KEY, NICHES, MAX_CHANNELS_PER_NICHE)",
   "id": "cc643db378a2ed3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['UCy0tKL1T7wFoYcxCe0xjN6Q', 'UCAL3JXZSzSm8AlZyD3nQdBA', 'UC4Xt1e33nqi6udIjVEJa8Nw', 'UCKWaEZ-_VweaEx1j62do_vQ', 'UCUuvZ0skL2WYZ3rhPMZbfdA', 'UCrM7B7SL_g1edFOnmj-SDKg', 'UCf9phz2kU2DaJBwDATTD05Q', 'UC9MQp8a5uhaIosZPHaoqEXQ', 'UCYUPLUCkMiUgiyVuluCc7tQ', 'UCYVU6rModlGxvJbszCclGGw', 'UCz6PEeVLG1TL6jMRTvSLm4g', 'UCKnaflu7tLEm2S5yHBYfmtg', 'UC4Tklxku1yPcRIH0VVCKoeA', 'UCiz26UeGvcTy4_M3Zhgk7FQ', 'UCF8H7dYHK6AvJF0EVonO3cw', 'UC3ByF8DcZ3yxUs7VP1NOuyA', 'UCFnKqecC7F2HTnWZlEzExbw', 'UC0QRG-gMqHhF5-BPdmJfs1w', 'UCRzScB0a-dc6OCYYZlP9qpA', 'UC7FWqacDz5A5bhbjxXbRN2Q', 'UCTR22zaU51pb2P72JS8oJtg', 'UC1tVU8H153ZFO9eRsxdJlhA', 'UC08uERSqTFS3xSwXKMLrUVg', 'UC905GJVaT_10oSdZq8sDoRg', 'UCZff6I2okeH71I5jsE0LirQ', 'UCuEOSK8blSM6j5jxVp3ttnQ', 'UCMN0a7GHQnC6H74SmCGSmdw', 'UCWv7vMbMWH4-V0ZXdmDpPBA', 'UCBsAE4_Cvr7FGnF6KVw9JfA', 'UCfJyQ3P2k_SuqfxVdqIEQNw', 'UCpKb02FsH4WH4X_2xhIoJ1A', 'UC42pOSNg804f1wCcj7qL0mA', 'UCV7cZwHMX_0vk8DSYrS7GCg', 'UCvGwM5woTl13I-qThI4YMCg', 'UCLZIElNyubHOvbfudT7KS1A', 'UCCfqyGl3nq_V0bo64CjZh8g', 'UCKvjEXuc-tomU0sW_iBZ3fw', 'UCzOb0YO5lm4AAHXC5VMy3-g', 'UCC05kqMzbQUqpAf_kqeOx0A', 'UCrmnnE3yzpAyAuX_hRqyLdg', 'UCBKM3HATRILZyG9jGNJKGXQ', 'UCocTdfI3p1tKDQEbQzDWrPg', 'UCIZFrMwq1lXOA2899t7d5Aw', 'UCqAL_b-jUOTPjrTSNl2SNaQ', 'UCUGjG_06mOK_6vkHpUi1Y4Q'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T17:55:28.095059Z",
     "start_time": "2026-01-29T17:55:28.075022900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "channel_ids = list(channels.keys())\n",
    "channel_ids += ['UCgKFOz_KrMbmypWrawtzDQg']\n",
    "print(channel_ids.index('UCgKFOz_KrMbmypWrawtzDQg'))\n",
    "print(len(channel_ids))\n",
    "print(channel_ids)"
   ],
   "id": "2753fba7c186fcd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "46\n",
      "['UCy0tKL1T7wFoYcxCe0xjN6Q', 'UCAL3JXZSzSm8AlZyD3nQdBA', 'UC4Xt1e33nqi6udIjVEJa8Nw', 'UCKWaEZ-_VweaEx1j62do_vQ', 'UCUuvZ0skL2WYZ3rhPMZbfdA', 'UCrM7B7SL_g1edFOnmj-SDKg', 'UCf9phz2kU2DaJBwDATTD05Q', 'UC9MQp8a5uhaIosZPHaoqEXQ', 'UCYUPLUCkMiUgiyVuluCc7tQ', 'UCYVU6rModlGxvJbszCclGGw', 'UCz6PEeVLG1TL6jMRTvSLm4g', 'UCKnaflu7tLEm2S5yHBYfmtg', 'UC4Tklxku1yPcRIH0VVCKoeA', 'UCiz26UeGvcTy4_M3Zhgk7FQ', 'UCF8H7dYHK6AvJF0EVonO3cw', 'UC3ByF8DcZ3yxUs7VP1NOuyA', 'UCFnKqecC7F2HTnWZlEzExbw', 'UC0QRG-gMqHhF5-BPdmJfs1w', 'UCRzScB0a-dc6OCYYZlP9qpA', 'UC7FWqacDz5A5bhbjxXbRN2Q', 'UCTR22zaU51pb2P72JS8oJtg', 'UC1tVU8H153ZFO9eRsxdJlhA', 'UC08uERSqTFS3xSwXKMLrUVg', 'UC905GJVaT_10oSdZq8sDoRg', 'UCZff6I2okeH71I5jsE0LirQ', 'UCuEOSK8blSM6j5jxVp3ttnQ', 'UCMN0a7GHQnC6H74SmCGSmdw', 'UCWv7vMbMWH4-V0ZXdmDpPBA', 'UCBsAE4_Cvr7FGnF6KVw9JfA', 'UCfJyQ3P2k_SuqfxVdqIEQNw', 'UCpKb02FsH4WH4X_2xhIoJ1A', 'UC42pOSNg804f1wCcj7qL0mA', 'UCV7cZwHMX_0vk8DSYrS7GCg', 'UCvGwM5woTl13I-qThI4YMCg', 'UCLZIElNyubHOvbfudT7KS1A', 'UCCfqyGl3nq_V0bo64CjZh8g', 'UCKvjEXuc-tomU0sW_iBZ3fw', 'UCzOb0YO5lm4AAHXC5VMy3-g', 'UCC05kqMzbQUqpAf_kqeOx0A', 'UCrmnnE3yzpAyAuX_hRqyLdg', 'UCBKM3HATRILZyG9jGNJKGXQ', 'UCocTdfI3p1tKDQEbQzDWrPg', 'UCIZFrMwq1lXOA2899t7d5Aw', 'UCqAL_b-jUOTPjrTSNl2SNaQ', 'UCUGjG_06mOK_6vkHpUi1Y4Q', 'UCgKFOz_KrMbmypWrawtzDQg']\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee89465e08f43085"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
